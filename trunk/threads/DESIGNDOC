			+--------------------+
			|        CS 140      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Steve Lesser <sklesser@stanford.edu>
Josh Parnell <parnell@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

--- thread.h ---
struct thread // Added the following fields
  int64_t wakeup_time; /* Absolute time to wakeup thread if thread is asleep */
  struct list_elem sleep_elem; /* List element for sleep threads */

--- thread.c ---
static struct list sleep_list; /* List of processes currently sleeping */

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

thread_sleep is called with a parameter of the wake up time for the
current thread. thread_sleep then disables interrupts, sets its wakeup
time, inserts itself into the sleep_list and finally blocks.

In schedule() the sleep_list is traversed and threads eligible to be woken
up are removed from the sleep_list and unblocked. 

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

The only timer interrupt handle which has new work is when schedule
is called (every 4th tick) upon which sleep_list is iterated to
potentially wake up sleeping threads. Since the sleep_list is
ordered the traversal can be exited early as soon as a thread not eligible
to be removed is iterated on.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

We disable interrupts to calculate the absolute end time of the thread to
be put to sleep and additionally disable interrupts to insert the newly
updated thread into the sleep_list.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Insertion of the thread into sleep_list is an ordered operation which
could potentially have race conditions, but interrupts are disabled when
inserting to avoid them.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We chose to create a new sleep_list to ensure rapid wakeup checks on timer
interrupts and to make sure the system stays speedy. Without using the
extra ordered list we would have had to traverse more threads upon
scheduling to determine if any sleeping threads were newly eligible to be
unblocked. Additionally, using a list followed the existing design of
threads existing in multiple lists such as ready_list and all_list.

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

--- thread.c ---

struct thread
  int base_priority;                  /* Base priority, before considering
                                         priority donations. */
  struct list priority_donations;     /* Sorted list (high-to-low) of all
                                         priorities donated to this thread. */
  struct lock *waiting_on;            /* Lock that the thread is waiting
                                         to acquire. NULL if the thread is
                                         not waiting on a lock. */
  struct list_elem donation_elem;     /* List element for donating a
                                         priority to other threads. */

--- synch.c ---

struct semaphore_elem
  int priority;                       /* Priority of this waiting thread */

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

We chose to embed a sorted linked-list of priority donations into each thread.
This means that, if thread A needs to donate a priority to thread B, A
will add its donation_elem to the priority_donations list of B. That way,
B keeps track of ALL donations given to it at any point in time. Additionally,
when a thread donates, it calls thread_donate_priority () on the receiving
thread (that is, thread A in our example would call
thread_donate_priority (B)). This call allows the thread that received the
donation to update its effective priority, as well as to update the priority
of any outstanding donation that it has made to another thread.

When a thread releases a lock, it iterates over the list of threads that were
waiting on the lock, and invokes thread_recall_donation () on each of the
threads, instructing them to each remove their donation_elem. This allows a
thread to give up any priority that it received as a result of holding the lock
that it just released, while keeping intact any donations that it has received
as a result of *other* locks that it still holds. In other words, clearing the
list of priority donations in lock_release () is *not* sufficient.

As an example, suppose that thread B is waiting on thread C's lock, in addition
to A waiting on B. To make the example more interesting, suppose that yet
another thread D is waiting on a *different* lock that thread C also holds.

In this complex case of priority donation, the donation lists would look like
this for the four threads:

  priority_donations (thread A)
    (empty list) 

  priority_donations (thread B)
    A.donation_elem
   
  priority_donations (thread C)
    B.donation_elem
    D.donation_elem
    
  priority_donations (thread D)
    (empty list)
  
Now, we point out that, when thread A donated priority to thread B, it called
thread_donate_priority (B), which would have caused B to recompute its
effective priority (taking into consideration the donation from A), *as
well as* to update any existing donation that B had made. In this way, the
priority from A would be transferred all the way to C *through* thread B's
donation_elem (which would hold the effective priority of B, taking into
account thread A's donation). This update allows chained donation to work
correctly.

Finally, we remark that, since we always keep a complete list of donations,
thread C will still have the correct priority if, for example, it releases
the lock upon which B was waiting, but not the lock upon which D was waiting.
This would cause thread C to invoke thread_recall_donation (B), so that
B.donation_elem would be removed from C.priority_donations, but D.donation_elem
will remain (which is the correct behavior, since C still holds a lock on
which D is waiting). Thus, the effective priority of C will be recomputed,
taking into consideration only the base priority of C and the donation_elem
of D.
  
---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

For locks and semaphores, we simply modified sema_up to use the list_min
function to return the highest-priority thread in the list of waiting
threads. Note that we used list_min rather than list_max because our
comparison function uses > rather than < (this choice was made so that
the threads in the ready list are ordered from highest-priority to
lowest-priority).

For condition variables, we added a new field to struct semaphore_elem to
store the priority of the thread to which the semaphore_elem belongs. Then,
when a thread calls cond_wait, we changed the list insertion to use
list_insert_ordered, such that the list will remain sorted from high-priority
threads to low-priority threads. Thus, when we want to wake up the highest-
priority thread, we need only grab the first element of the list.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

Suppose thread A is trying to acquire lock L1, which is held by thread B,
which is trying to acquire lock L2, which is held by thread C. this is a
classic example of nested donation. The call stack during the call to
lock_acquire would look something like this:

  --- Thread A ---
  lock_acquire (L1)
    > thread_donate_priority (A)
      --- Thread B ---
      > thread_update_priority (B)
      > thread_donate_priority (B)
        --- Thread C ---
        > thread_update_priority (C)
      
When A tries to acquire L1, A recognizes that L1 is held by B. A sets
its internal waiting_on pointer to L1, and then calls thread_donate_priority
on itself. Thread_donate_priority recomputes A's effective priority (strictly
speaking, this is not necessary, but the code reuse that comes with designing
the function this way is worth the small amount of extra work). The function
then recognizes that A is waiting on L1, looks into L1 and sees that B holds
L1, and then makes a donation to B. That is, thread_donate_priority inserts
the donation_elem of A into the ordered donation list of B. Finally, thread_
donate_priority calls itself recursively on B.

The same sequence of events then happens with B: B updates its effective
priority (taking into consideration the new donation_elem that it received
from A), recognizes that it is waiting on L2, which is held by C, then updates
the donation_elem that it initially gave to C. It does so be calling
thread_recall_donation (B). Note that A does not do this because A is the
running thread, and, as such, knows that it had not already given a donation.
B then re-inserts its donation_elem into C's donation list. This step of
removing and re-inserting is crucial to preserve the sorted property of
donation lists. B then calls thread_donate_priority recursively on C.

Finally, C updates its effective priority, then, recognizing that it is not
waiting on a lock, terminates the call and unwinds the stack.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

As described above,

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0
 4
 8
12
16
20
24
28
32
36

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?
